import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client

# K·∫øt n·ªëi Supabase
url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
supabase = create_client(url, key)

def clean_price(price_val):
    try:
        if price_val is None: return 0.0
        clean_str = "".join(filter(str.isdigit, str(price_val)))
        return float(clean_str) if clean_str else 0.0
    except:
        return 0.0

def save_gold(source_code, gold_type, buy, sell, web_url):
    data = {
        "source_code": source_code,
        "gold_type": gold_type,
        "buy_price": buy,
        "sell_price": sell,
        "source_url": web_url,
        "updated_at": "now()"
    }
    try:
        supabase.table("gold_prices").upsert(data, on_conflict="source_code,gold_type").execute()
        print(f"‚úÖ [{source_code}] {gold_type}: {buy} - {sell}")
    except Exception as e:
        print(f"‚ùå L·ªói l∆∞u {source_code}: {e}")

# 1. B·∫¢O T√çN M·∫†NH H·∫¢I
def crawl_btmh():
    print("üöÄ ƒêang c√†o B·∫£o T√≠n M·∫°nh H·∫£i...")
    target_url = "https://baotinmanhhai.vn/gia-vang-hom-nay"
    try:
        res = requests.get(target_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table tr')
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 3:
                name = cols[0].get_text(strip=True)
                if any(x in name for x in ["SJC", "Kim Gia B·∫£o", "Nh·∫´n Tr√≤n"]):
                    save_gold("BTMH", name, clean_price(cols[1].text), clean_price(cols[2].text), target_url)
    except: print("L·ªói BTMH")

# 2. DOJI (S·ª≠ d·ª•ng selector ch√≠nh x√°c h∆°n cho DOJI H√† N·ªôi)
def crawl_doji():
    print("üöÄ ƒêang c√†o DOJI...")
    target_url = "https://giavang.doji.vn/"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        res = requests.get(target_url, headers=headers, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # DOJI th∆∞·ªùng b·ªçc d·ªØ li·ªáu trong c√°c h√†ng tr
        rows = soup.find_all('tr')
        count = 0
        for row in rows:
            cols = row.find_all(['td', 'th'])
            if len(cols) >= 3:
                name = " ".join(cols[0].get_text().split())
                # T·∫≠p trung v√†o SJC v√† c√°c lo·∫°i v√†ng ph·ªï bi·∫øn c·ªßa DOJI
                if any(x in name.upper() for x in ["SJC", "DOJI", "N·ªÆ TRANG 99.9"]):
                    buy = clean_price(cols[1].get_text())
                    sell = clean_price(cols[2].get_text())
                    if buy > 1000000:
                        save_gold("DOJI", name, buy, sell, target_url)
                        count += 1
        if count == 0: print("‚ö†Ô∏è DOJI: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu.")
    except: print("L·ªói DOJI")

# 3. PH√ö QU√ù
def crawl_phuquy():
    print("üöÄ ƒêang c√†o Ph√∫ Qu√Ω...")
    target_url = "https://phuquygroup.vn/"
    try:
        res = requests.get(target_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table tr')
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 3:
                name = cols[0].get_text(strip=True)
                if any(x in name for x in ["SJC", "Ph√∫ Qu√Ω"]):
                    save_gold("PHUQUY", name, clean_price(cols[1].text), clean_price(cols[2].text), target_url)
    except: print("L·ªói Ph√∫ Qu√Ω")

# 4. B·∫¢O T√çN MINH CH√ÇU (S·ª≠a d√πng html.parser ƒë·ªÉ kh√¥ng c·∫ßn c√†i th√™m lxml)
def crawl_btmc():
    print("üöÄ ƒêang g·ªçi API B·∫£o T√≠n Minh Ch√¢u (XML)...")
    api_url = "http://api.btmc.vn/api/BTMCAPI/getpricebtmc?key=3kd8ub1llcg9t45hnoh8hmn7t5kc2v"
    try:
        res = requests.get(api_url, timeout=20)
        # S·ª≠ d·ª•ng 'html.parser' thay v√¨ 'xml' ƒë·ªÉ tr√°nh l·ªói thi·∫øu th∆∞ vi·ªán lxml
        soup = BeautifulSoup(res.content, 'html.parser')
        data_tags = soup.find_all('data') # html.parser t·ª± vi·∫øt th∆∞·ªùng c√°c tag
        
        count = 0
        for tag in data_tags:
            row_idx = tag.get('row')
            name = tag.get(f'n_{row_idx}')
            buy = tag.get(f'pb_{row_idx}')
            sell = tag.get(f'ps_{row_idx}')
            
            if name and any(x in name for x in ["SJC", "V√†ng R·ªìng ThƒÉng Long", "Nh·∫´n Tr√≤n"]):
                save_gold("BTMC", name, clean_price(buy), clean_price(sell), "https://btmc.vn")
                count += 1
        if count == 0: print("‚ö†Ô∏è BTMC: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p.")
    except Exception as e:
        print(f"‚ùå L·ªói API BTMC: {e}")

if __name__ == "__main__":
    crawl_btmh()
    crawl_doji()
    crawl_phuquy()
    crawl_btmc()
