import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client

# Káº¿t ná»‘i Supabase
url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
supabase = create_client(url, key)

def clean_price(price_val):
    try:
        if not price_val: return 0.0
        # Láº¥y táº¥t cáº£ chá»¯ sá»‘
        clean_str = "".join(filter(str.isdigit, str(price_val)))
        return float(clean_str) if clean_str else 0.0
    except:
        return 0.0

def save_gold(source_code, gold_type, buy, sell, web_url):
    data = {
        "source_code": source_code,
        "gold_type": gold_type,
        "buy_price": buy,
        "sell_price": sell,
        "source_url": web_url,
        "updated_at": "now()"
    }
    try:
        supabase.table("gold_prices").upsert(data, on_conflict="source_code,gold_type").execute()
        print(f"âœ… [{source_code}] {gold_type}: {buy} - {sell}")
    except Exception as e:
        print(f"âŒ Lá»—i lÆ°u {source_code}: {e}")

# HÃ€M Tá»”NG Há»¢P CÃ€O Tá»ª WEBGIA (Láº¥y DOJI, BTMC, SJC, PHÃš QUÃ)
def crawl_webgia():
    print("ðŸš€ Äang cÃ o dá»¯ liá»‡u tá»•ng há»£p tá»« WebGia...")
    # Trang nÃ y tá»•ng há»£p giÃ¡ vÃ ng ráº¥t sáº¡ch
    target_url = "https://webgia.com/gia-vang/sjc/" 
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        res = requests.get(target_url, headers=headers, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # TÃ¬m táº¥t cáº£ cÃ¡c báº£ng giÃ¡
        tables = soup.find_all('table', class_='table-price')
        
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 3:
                    name_raw = cols[0].get_text(strip=True)
                    buy = clean_price(cols[1].get_text(strip=True))
                    sell = clean_price(cols[2].get_text(strip=True))
                    
                    # PhÃ¢n loáº¡i dá»¯ liá»‡u dá»±a trÃªn tÃªn hÃ ng
                    # DOJI
                    if "DOJI" in name_raw.upper():
                        save_gold("DOJI", name_raw, buy, sell, target_url)
                    
                    # Báº¢O TÃN MINH CHÃ‚U
                    elif "MINH CHÃ‚U" in name_raw.upper() or "BTMC" in name_raw.upper():
                        save_gold("BTMC", name_raw, buy, sell, target_url)
                        
                    # PHÃš QUÃ
                    elif "PHÃš QUÃ" in name_raw.upper():
                        save_gold("PHUQUY", name_raw, buy, sell, target_url)
                        
    except Exception as e:
        print(f"âŒ Lá»—i crawl WebGia: {e}")

# 1. Báº¢O TÃN Máº NH Háº¢I (Trang nÃ y váº«n cÃ o trá»±c tiáº¿p Ä‘Æ°á»£c vÃ¬ cáº¥u trÃºc á»•n)
def crawl_btmh():
    print("ðŸš€ Äang cÃ o Báº£o TÃ­n Máº¡nh Háº£i...")
    target_url = "https://baotinmanhhai.vn/gia-vang-hom-nay"
    try:
        res = requests.get(target_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table tr')
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 3:
                name = cols[0].get_text(strip=True)
                if any(x in name for x in ["SJC", "Kim Gia Báº£o", "Nháº«n TrÃ²n"]):
                    save_gold("BTMH", name, clean_price(cols[1].text), clean_price(cols[2].text), target_url)
    except: print("Lá»—i BTMH")

if __name__ == "__main__":
    # Æ¯u tiÃªn cÃ o WebGia Ä‘á»ƒ láº¥y DOJI, BTMC, PHUQUY
    crawl_webgia()
    # Máº¡nh Háº£i cÃ o riÃªng
    crawl_btmh()
